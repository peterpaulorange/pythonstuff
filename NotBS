#!/usr/bin/python

import os, requests, re, shutil
import click
from requests_html import HTMLSession  # Cheat, helps to render pages for javascript


@click.command()
@click.option("--url")
def get_img(url):
    try:
        url = url.rstrip("/")
    except AttributeError:
        print("URL is not specified")
        exit()
    session = HTMLSession()
    r = session.get(url)
    r.html.render()  # Attempt to bypass some scraping protection by rendering page for JS execution
    links = img_links(r.html)  # And i can use cheats =P
    link_count = len(links)
    # print("\n".join(links))
    print("Found " + str(link_count) + " images!")
    for img in links:
        download_img(url, img)
    print("DONE! Downloaded images: " + str(link_count))


# This function uses requests-html functionality to search for a tags
def img_links(html):
    def gen():
        for img_link in html.find("img"):
            try:
                src = img_link.attrs["src"].strip()
                print("SRC: " + src)
                if (
                    src.endswith("jpg")
                    or src.endswith("jpeg")
                    or src.endswith("png")
                    or src.endswith("gif")
                ):
                    yield src
            except requests.exceptions.MissingSchema:
                print("Invalid URL " + img_link)
                print("Terminating")

    return set(gen())


def process_url(url, img_url):
    img_url = img_url.strip()
    result_url = img_url
    if img_url.startswith("http"):
        return result_url
    else:
        if img_url.startswith("//"):
            result_url = "https:" + img_url
            # print(result_url)
        else:
            if img_url.startswith("/"):
                result_url = url + img_url
            else:
                print("Invalid image URL!" + result_url)
    return result_url


def get_name(img_url):
    return str(re.findall(r"[^\/]+$", img_url)).strip("[']\"")


def download_img(url, img_url):
    result_img_url = process_url(url, img_url)
    response = requests.get(result_img_url, stream=True)
    if response.status_code == 200:
        os.makedirs(
            "images/", exist_ok=True
        )  # Create directory for images, skip if exists
        img_name = get_name(img_url)
        print("Downloading " + result_img_url)
        with open("images/" + img_name, "wb") as out_img:
            shutil.copyfileobj(response.raw, out_img)  # Save image
        del response
        print("OK")
    else:
        print(response.status_code + "\nFAILED TO DOWNLOAD IMAGE: " + result_img_url)


if __name__ == "__main__":
    get_img()
    
    ---///---
    import urllib, csv, requests, os
from pathlib import Path


spreadsheetAddress = 'C:\\SOURCE\\CSV\\FILE.csv'
targetDirectory = 'C:\\TARGET\\IMAGE\\SAVE\\LOCATION\\'

def getSpreadsheetContents(spreadsheetAddress):
    with open(spreadsheetAddress) as csvfile:
        readCSV = csv.reader(csvfile, delimiter=',')
        imageSet = {}
        for row in readCSV:
            if 'image_id' not in row:
                imageSet[row[0]] = row[1]
    return imageSet


if __name__ == "__main__":
    if os.path.exists(spreadsheetAddress) and os.path.exists(targetDirectory):
        imageDict = getSpreadsheetContents(spreadsheetAddress)
        for key, value in imageDict.items():
            if requests.get(value).status_code == 200:
                filename, file_extension = os.path.splitext(value)
                address = str(targetDirectory + "\\" + key + file_extension)
                urllib.request.urlretrieve(value, address)
    else:
        raise Exception("File not found")
        
        --//--
       import io
import pathlib
import hashlib
import pandas as pd
import requests
from bs4 import BeautifulSoup
from PIL import Image
from selenium import webdriver


def get_content_from_url(url):
   driver = webdriver.Chrome()  # add "executable_path=" if driver not in running directory
   driver.get(url)
   driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
   page_content = driver.page_source
   driver.quit()  # We do not need the browser instance for further steps.
   return page_content


def parse_image_urls(content, classes, location, source):
   soup = BeautifulSoup(content)
   results = []
   for a in soup.findAll(attrs={"class": classes}):
       name = a.find(location)
       if name not in results:
           results.append(name.get(source))
   return results


def save_urls_to_csv(image_urls):
   df = pd.DataFrame({"links": image_urls})
   df.to_csv("links.csv", index=False, encoding="utf-8")


def get_and_save_image_to_file(image_url, output_dir):
   response = requests.get(image_url, headers={"User-agent": "Mozilla/5.0"})
   image_content = response.content
   image_file = io.BytesIO(image_content)
   image = Image.open(image_file).convert("RGB")
   filename = hashlib.sha1(image_content).hexdigest()[:10] + ".png"
   file_path = output_dir / filename
   image.save(file_path, "PNG", quality=80)


def main():
   url = "https://your.url/here?yes=brilliant"
   content = get_content_from_url(url)
   image_urls = parse_image_urls(
       content=content, classes="blog-card__link", location="img", source="src",
   )
   save_urls_to_csv(image_urls)

   for image_url in image_urls:
       get_and_save_image_to_file(
           image_url, output_dir=pathlib.Path("nix/path/to/test"),
       )


if __name__ == "__main__":  #only executes if imported as main file
   main()

----///---
